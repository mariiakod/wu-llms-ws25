{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f482b875-9f53-4915-9898-4b62f8415623",
   "metadata": {},
   "source": [
    "# **Detecting and Reducing Toxicity in Dota 2 Chat Using Machine Learning and Large Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219b6f6-a093-4ce9-9dae-944ea4958055",
   "metadata": {},
   "source": [
    "### *Authors:*  \n",
    "**Mariia Petryk & Mariia Kodolova**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1765c51-425c-423b-aaf1-cc8402e9f4af",
   "metadata": {},
   "source": [
    "##  **Project Overview**\n",
    "\n",
    "Online games like **Dota 2** often include toxic messages.  \n",
    "This project aims to detect and reduce toxic messages in Dota 2 player chat.\n",
    "We fine-tune a RoBERTa classifier on Dota 2 chat data, compare it to a generic toxicity model and a zero-shot LLM (Llama 3), and finally build a detoxification pipeline that rewrites toxic messages into friendly ones.\n",
    "\n",
    "## **Goals**\n",
    "- Classify chat messages into non-toxic / mild / toxic.\n",
    "- Compare different model families.\n",
    "- Test if toxicity decreases after LLM rewriting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad07b17-3dba-4bfd-ad4d-e8b1aa64ee4c",
   "metadata": {},
   "source": [
    " **Project video**: https://www.youtube.com/watch?v=BdTYQJnZwak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f8369d9-d922-418a-833c-ab3d7ded63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15270) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15273) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (4.45.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/llm-course/lib/python3.10/site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers.utils.hub as tr_hub\n",
    "import requests\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184b2cd-7c43-4410-a197-32b6fa10861e",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d6bc9-400b-4202-a43d-afe0c2160f24",
   "metadata": {},
   "source": [
    "#### 1. Load the data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8f427cd-78bc-499e-99cc-70e3a6b01db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1722, 2)\n",
      "Validation shape: (192, 2)\n",
      "Test shape: (638, 2)\n"
     ]
    }
   ],
   "source": [
    "# Download file from Hugging Face\n",
    "def download(url, filename):\n",
    "    r = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "download(\"https://huggingface.co/datasets/dffesalbon/dota-2-toxic-chat-data/resolve/main/train.csv\", \"train.csv\")\n",
    "download(\"https://huggingface.co/datasets/dffesalbon/dota-2-toxic-chat-data/resolve/main/validation.csv\", \"validation.csv\")\n",
    "download(\"https://huggingface.co/datasets/dffesalbon/dota-2-toxic-chat-data/resolve/main/test.csv\", \"test.csv\")\n",
    "\n",
    "# We download the 3 splits (train/validation/test) and read them into Pandas\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df   = pd.read_csv(\"validation.csv\")\n",
    "test_df  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d18de1-4185-45ed-90cc-ff8d1d27cf67",
   "metadata": {},
   "source": [
    "#### 2. Look at label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69cfecaa-edaa-4a9d-890b-29bd2b565b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = Counter(train_df[\"target\"])\n",
    "plt.bar(label_counts.keys(), label_counts.values(), color=['green','red','orange'])\n",
    "plt.xlabel(\"Label (0=non-toxic, 1=mild, 2=toxic)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Label Distribution (Train set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03835425-9af0-4b45-ad1f-98f4259a85a8",
   "metadata": {},
   "source": [
    "The training data is imbalanced: non-toxic messages are the majority class, while toxic messages are less frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db50bf-07aa-4360-a7e0-6e32a4abf110",
   "metadata": {},
   "source": [
    "#### 3. Quick qualitative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b35af2f-6c87-41e9-ba4a-e65c057a8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples for label 0:\n",
      "- can t win alone\n",
      "- buy more hearts rofl\n",
      "- no one ever gives up\n",
      "\n",
      "Examples for label 1:\n",
      "- he is best shittalker au\n",
      "- really retard\n",
      "- wahtthefc\n",
      "\n",
      "Examples for label 2:\n",
      "- why u guys so smell and poor country  \n",
      "- cyka\n",
      "- got fucked by  k scrub\n"
     ]
    }
   ],
   "source": [
    "for label in [0, 1, 2]:\n",
    "    print(f\"\\nExamples for label {label}:\")\n",
    "    i = 0\n",
    "    for row in train_df.itertuples():\n",
    "        if row.target == label:\n",
    "            print(\"-\", row.message)\n",
    "            i += 1\n",
    "        if i >= 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38e022-5e1e-4d8b-be96-ceb0c0340793",
   "metadata": {},
   "source": [
    "We see that label 0 corresponds to friendly/neutral chat, label 1 captures mild trash-talk, and label 2 contains insults and clearly toxic language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e6efb-0a40-4a41-a542-e6f9c1b36be1",
   "metadata": {},
   "source": [
    "#### 4. Wrap this into a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "729f111a-8fce-42f2-9ad9-31dd0899a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotaToxicDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.texts = dataframe[\"message\"].tolist()\n",
    "        self.targets = dataframe[\"target\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.targets[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043298dd-33e6-4328-a206-1dd0f9b7f9c1",
   "metadata": {},
   "source": [
    "#### 5. Create train/val/test datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b4ddfb5-0cfa-47a4-b438-a89f8d0ea219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DotaToxicDataset(train_df)\n",
    "val_dataset   = DotaToxicDataset(val_df)\n",
    "test_dataset  = DotaToxicDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89e31731-2b66-4f8f-8960-e1e976211910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch texts example: ('fair enough man', 'yaaaaa we was baiting yr mom to sleep with yr mouse ')\n",
      "Batch labels example: tensor([0, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_texts, batch_labels = next(iter(train_loader))\n",
    "print(\"Batch texts example:\", batch_texts[:2])\n",
    "print(\"Batch labels example:\", batch_labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c63f1-068e-4bc2-a340-abc6668109fc",
   "metadata": {},
   "source": [
    "Our DataLoader now provides mini-batches of raw chat messages and their toxicity labels, ready to be passed into a tokenizer and a model (RoBERTa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737022f4-dce5-4295-b868-fbe81ebe5ee2",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization and Batch Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c76eb4-060d-4824-a016-8864337b6f9e",
   "metadata": {},
   "source": [
    "#### 1. Import the RoBERTa tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6ba0d-4c73-41c2-b408-cded683144a9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "In the lectures, the tokenizer was loaded with a simple line:\n",
    " *tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")*\n",
    "\n",
    "It automatically downloads the model and all its configuration files from Hugging Face. We tried to do the same with ROBERTa.\n",
    "However, in our environment, this standard method caused an error because the current version of the transformers library tries to access a non-existent folder called additional_chat_templates on the Hugging Face Hub.\n",
    "That leads to a 404 Not Found error.\n",
    "\n",
    "To fix the problem, we added a small patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "232fd01f-7a77-4ada-bc7c-59d698247f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_list_repo_templates(*args, **kwargs):\n",
    "    return []\n",
    "\n",
    "tr_hub.list_repo_templates = safe_list_repo_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c499698-e7a7-4aa0-b6e0-64cb8b69ff48",
   "metadata": {},
   "source": [
    "→ We define a new function with the same signature as the original one.\n",
    "It does not call the internet, it just returns an empty list.\n",
    "For RoBERTa this is fine, because it doesn’t need any chat templates.\n",
    "\n",
    "→ We replace the original list_repo_templates function with our safe version.\n",
    "So when AutoTokenizer.from_pretrained(\"roberta-base\") calls it, it no longer makes the broken request and the tokenizer loads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27683226-11a1-47c9-a0e1-2f486833aed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm-course/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"   \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ac86f-fac6-4908-936e-75be067f5e47",
   "metadata": {},
   "source": [
    "#### 2. Collate function (tokenizes every batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "751e22f9-1fc1-4033-bb19-ee2bd8b232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    enc = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = torch.tensor(labels)\n",
    "    return enc, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190adc9-d2f8-46f0-b8f7-2b25eb52728c",
   "metadata": {},
   "source": [
    "#### 3. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4e84e2a-6d2a-45bb-9bb7-fc88988cbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb9f4a-f9f5-452f-b792-2b33fda13f18",
   "metadata": {},
   "source": [
    "#### 4. Test that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5967cafa-ca7d-4264-9311-9a53677d6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n",
      "torch.Size([16, 10])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "enc, labels = next(iter(train_loader))\n",
    "\n",
    "print(enc[\"input_ids\"].shape)\n",
    "print(enc[\"attention_mask\"].shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e9058-a3cd-407a-929b-d8edde2c04f1",
   "metadata": {},
   "source": [
    "### Step 3: Load and Fine-Tune RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40890bbb-45a7-4ebb-add3-eb8e0d3b2781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaecf2-73a2-4751-ac1c-83c3b66d568f",
   "metadata": {},
   "source": [
    "#### 1. Load RoBERTa for classification (3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c0858ac-5a9e-4a3e-94a6-ebd65bb61717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=3 # 0 = non-toxic, 1 = mild, 2 = toxic\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36017c-b58c-493f-ba1d-c579e0336932",
   "metadata": {},
   "source": [
    "#### 2. Set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95fbae23-486c-4cb6-b28f-d4a454d111c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4b970-18b4-4640-93ed-554b80c3292f",
   "metadata": {},
   "source": [
    "AdamW is the standard optimizer for transformer fine-tuning. A learning rate of 2e-5 is typical and stable for RoBERTa. This optimizer will update the model weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13581713-15b6-4c7c-8ae8-926f9c00e9be",
   "metadata": {},
   "source": [
    "#### 3. Training loop (1 epoch to start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69c3ccca-342c-4833-abef-a79147704c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for encodings, labels in tqdm(data_loader):\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**encodings, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4583d-7758-4b96-832f-00c6e72d3447",
   "metadata": {},
   "source": [
    "The model gets batches of tokenized data, computes the loss, backpropagates, and updates its weights.\n",
    "We accumulate the loss so we can report an average at the end of the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbf1b1-2998-408b-9c1a-396aad3d0e79",
   "metadata": {},
   "source": [
    "#### 4. Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f50a8797-ac02-4387-89c4-de1b2cec1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encodings, labels in data_loader:\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**encodings)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e4a7c-eec1-4189-90ca-14ea2e596242",
   "metadata": {},
   "source": [
    "We evaluate the model without training it.\n",
    "The model outputs logits, we take the class with the highest score, and then compute accuracy.\n",
    "This gives us a simple initial performance measure before we compute full metrics like F1 later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6a478-5e3f-49c7-95a5-63818434a968",
   "metadata": {},
   "source": [
    "#### 5. Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4a9fc9e-78e6-45e8-aba0-1f305154353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [28:55<00:00, 16.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7943680242255882\n",
      "Validation accuracy: 0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "train_loss = train_epoch(model, train_loader, optimizer)\n",
    "val_acc = evaluate(model, val_loader)\n",
    "\n",
    "print(\"Train loss:\", train_loss)\n",
    "print(\"Validation accuracy:\", val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a4bc8-1af6-4e0a-9af4-940e79e5c801",
   "metadata": {},
   "source": [
    "After one epoch of fine-tuning RoBERTa on the Dota 2 toxicity dataset, we obtain a training loss of 0.86 and a validation accuracy of 68.75%, which is clearly above random guessing (1/3 = 33%). This shows that the model learns to distinguish between the three toxicity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a79fe-3df7-4d4d-a398-536de157f84e",
   "metadata": {},
   "source": [
    "#### 6. Train for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31877f6c-8fee-4621-aaca-b664877f8eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [05:52<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.4396827996329025\n",
      "  Validation accuracy: 0.7916666666666666\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [05:32<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.32187875149840556\n",
      "  Validation accuracy: 0.84375\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [05:32<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.26196175323868237\n",
      "  Validation accuracy: 0.828125\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(\"  Train loss:\", train_loss)\n",
    "    print(\"  Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d0390-f394-4d39-bf28-6deec47cc731",
   "metadata": {},
   "source": [
    "The RoBERTa model improves quickly during fine-tuning. Training loss decreases consistently across epochs (0.52 → 0.36 → 0.26), and validation accuracy peaks at 82.3% in Epoch 2. The small drop in Epoch 3 indicates early overfitting, so Epoch 2 represents the best-performing checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420b378-a75d-483d-96eb-cc0fc1d5502c",
   "metadata": {},
   "source": [
    "### Step 4: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7f04f-e5a3-4271-ad99-890627bd81b5",
   "metadata": {},
   "source": [
    "#### 1. Create a DataLoader for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7219d88f-1b00-47bd-9556-508f7a2391a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa8737-1d08-4cd0-b1fc-5a4fcf76bd5b",
   "metadata": {},
   "source": [
    "We create a DataLoader so the test set can be passed to the model in batches.\n",
    "We don’t shuffle because test evaluation must be deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b93c98-8d26-41cc-854b-2c310d7d169d",
   "metadata": {},
   "source": [
    "#### 2. Compute predictions on the test set + full metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "02b6455c-4be4-438f-a8d3-2efd3e989fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encodings, labels in data_loader:\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**encodings)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d38e3-2c24-4c0b-be33-78ab929de855",
   "metadata": {},
   "source": [
    "We collect all predictions and true labels.\n",
    "Then we calculate accuracy, precision, recall, and weighted F1-score.\n",
    "Weighted F1 is used because our dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "df4ac3bc-6290-4f00-92c2-ac73b3656ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8275862068965517\n",
      "Test Precision: 0.8379793747667976\n",
      "Test Recall: 0.8275862068965517\n",
      "Test F1: 0.8301532186403101\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_prec, test_rec, test_f1 = evaluate_test(model, test_loader)\n",
    "\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Test Precision:\", test_prec)\n",
    "print(\"Test Recall:\", test_rec)\n",
    "print(\"Test F1:\", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6043a-3680-4edf-936a-7019b5397f71",
   "metadata": {},
   "source": [
    "The final evaluation on the test set shows that the fine-tuned RoBERTa model achieves 82% accuracy, with balanced precision (0.83), recall (0.82), and F1-score (0.82). Since the scores are very similar, the model performs consistently across all classes and generalizes well to unseen data. These results confirm that the fine-tuned RoBERTa classifier is effective for detecting toxicity levels in Dota 2 chat messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ff0cc-5bc0-4f9f-8bdc-794666b724ef",
   "metadata": {},
   "source": [
    "### Step 5: Compare with a generic toxicity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78b9a6-41c5-4f8d-a317-808cd402292e",
   "metadata": {},
   "source": [
    "We converted the dataset to a binary version because the generic toxicity model only supports two classes, and binary labels allow us to directly compare its performance with our fine-tuned Dota-specific classifier under equal conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc44b3-63f9-4078-9e40-2b1b92f58b06",
   "metadata": {},
   "source": [
    "#### 1. Install and load the generic toxicity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d902af68-62eb-457c-8aaf-3cb740844d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a6e368fb-cddb-43bb-af06-a58467cb25f2)')' thrown while requesting HEAD https://huggingface.co/s-nlp/roberta_toxicity_classifier/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "/opt/miniconda3/envs/llm-course/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tox_model_name = \"s-nlp/roberta_toxicity_classifier\"\n",
    "tox_tokenizer = AutoTokenizer.from_pretrained(tox_model_name)\n",
    "tox_model = AutoModelForSequenceClassification.from_pretrained(tox_model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda6dea-9575-4ea4-8a6c-1fcc4b17a54d",
   "metadata": {},
   "source": [
    "#### 2. Prepare a binary version of our labels for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0408b30d-e1b8-4331-baf0-0365d8692794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary_label(x):\n",
    "    # 0 -> 0 (non-toxic)\n",
    "    # 1,2 -> 1 (toxic)\n",
    "    return 0 if x == 0 else 1\n",
    "\n",
    "test_labels_3 = np.array([label for _, label in test_dataset])\n",
    "test_labels_bin = np.array([to_binary_label(x) for x in test_labels_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b803b3-dce6-4da9-8b3f-026b72e0c1cc",
   "metadata": {},
   "source": [
    "We create a NumPy array with binary labels, where both mild and strong toxicity count as “toxic” (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efe284-50fe-41c6-a055-fa241710c65a",
   "metadata": {},
   "source": [
    "#### 3. Evaluate our fine-tuned RoBERTa in binary mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "809a53cd-beef-4f3a-9b04-341afc116548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own model (binary) - Accuracy: 0.9420062695924765\n",
      "Own model (binary) - Precision: 0.9246575342465754\n",
      "Own model (binary) - Recall: 0.9473684210526315\n",
      "Own model (binary) - F1: 0.9358752166377816\n"
     ]
    }
   ],
   "source": [
    "def predict_binary_with_own_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds_bin = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for enc, labels in data_loader:\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            outputs = model(**enc)\n",
    "            preds_3 = outputs.logits.argmax(dim=1).cpu().numpy()\n",
    "            preds_bin = np.array([to_binary_label(x) for x in preds_3])\n",
    "            all_preds_bin.extend(preds_bin)\n",
    "\n",
    "    return np.array(all_preds_bin)\n",
    "\n",
    "own_bin_preds = predict_binary_with_own_model(model, test_loader)\n",
    "\n",
    "own_acc = accuracy_score(test_labels_bin, own_bin_preds)\n",
    "own_prec, own_rec, own_f1, _ = precision_recall_fscore_support(\n",
    "    test_labels_bin, own_bin_preds, average=\"binary\"\n",
    ")\n",
    "\n",
    "print(\"Own model (binary) - Accuracy:\", own_acc)\n",
    "print(\"Own model (binary) - Precision:\", own_prec)\n",
    "print(\"Own model (binary) - Recall:\", own_rec)\n",
    "print(\"Own model (binary) - F1:\", own_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00d3b6-c87f-47c9-9192-4faf1ff53456",
   "metadata": {},
   "source": [
    "After converting the task to binary toxicity detection, our fine-tuned RoBERTa achieved very strong results on the test set, with an accuracy of 92.9%, precision of 0.89, recall of 0.96, and an F1-score of 0.92. These numbers demonstrate that the model reliably recognizes toxic messages in Dota 2 chat and captures almost all toxic instances due to its high recall.\n",
    "\n",
    "The evaluation confirms that reducing the problem to two classes makes the task easier and therefore naturally yields higher scores than the 3-class setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2aa0a-b7b4-42bd-94b1-29e9b8c84ab2",
   "metadata": {},
   "source": [
    "### Step 6: Generic toxicity model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728bc74-5c44-4c22-bbd6-bf9739ab9cb8",
   "metadata": {},
   "source": [
    "#### 1. Load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "25faf6f4-2ec7-42a6-bf7d-904bf5d311b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm-course/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_name = \"s-nlp/roberta_toxicity_classifier\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(generic_name)\n",
    "gen_model = AutoModelForSequenceClassification.from_pretrained(generic_name)\n",
    "\n",
    "gen_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a8339-f041-448e-ad17-05c743ba7b64",
   "metadata": {},
   "source": [
    "#### 2. Prepare test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2970a1f9-2d18-490e-bd29-103638e1a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = non toxic, 1 = mild, 2 = toxic\n",
    "# → convert to 0 (non-toxic) or 1 (toxic)\n",
    "test_binary_labels = (test_df[\"target\"] > 0).astype(int).tolist()\n",
    "\n",
    "test_messages = test_df[\"message\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9660c7-ac5e-4ab6-8bca-c19ec4cc3c57",
   "metadata": {},
   "source": [
    "The generic model only has 2 outputs, so we transform the ground truth the same way:\n",
    "anything >0 becomes toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e135ab-6daa-4e50-ab00-4e4ea42f7c14",
   "metadata": {},
   "source": [
    "#### 3. Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "faf02925-2614-4c9c-9a99-c355723189a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "\n",
    "for msg in test_messages:\n",
    "    enc = gen_tokenizer(msg, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = gen_model(**enc)\n",
    "    pred = torch.argmax(out.logits, dim=1).item()\n",
    "    all_preds.append(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c12d8-0792-49b8-b3b5-9194fc2e5d0a",
   "metadata": {},
   "source": [
    "We feed each message into the model and get a toxic/non-toxic prediction for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe51ded-ea97-4445-b059-f3e7fbb9d0e7",
   "metadata": {},
   "source": [
    "### 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a527c8e-0dd3-4963-ae22-16848d90b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4d780d77-441c-4f6c-a899-40af84b24073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic model (binary) - Accuracy: 0.8652037617554859\n",
      "Generic model (binary) - Precision: 0.927038626609442\n",
      "Generic model (binary) - Recall: 0.7578947368421053\n",
      "Generic model (binary) - F1: 0.833976833976834\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(test_binary_labels, all_preds)\n",
    "prec = precision_score(test_binary_labels, all_preds)\n",
    "rec = recall_score(test_binary_labels, all_preds)\n",
    "f1 = f1_score(test_binary_labels, all_preds)\n",
    "\n",
    "print(\"Generic model (binary) - Accuracy:\", acc)\n",
    "print(\"Generic model (binary) - Precision:\", prec)\n",
    "print(\"Generic model (binary) - Recall:\", rec)\n",
    "print(\"Generic model (binary) - F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c86ee-132e-4e88-ab12-4033a3cfc6a6",
   "metadata": {},
   "source": [
    "The fine-tuned RoBERTa model performs substantially better than the generic toxicity classifier on Dota 2 chat. Fine-tuning on domain-specific data allows the model to understand gaming slang, abbreviations, and context that the generic model does not recognize. As a result, the fine-tuned model achieves higher accuracy, recall, and F1 score, confirming the importance of domain adaptation for toxicity detection in gaming environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511962f0-86a8-43b1-8200-59905a1848f2",
   "metadata": {},
   "source": [
    "## Step 7: Zero-shot baseline: LLAMA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71810b-b7ce-4e90-a37e-0a43963832c7",
   "metadata": {},
   "source": [
    "In this part of the project we use Llama 3 as zero-shot toxity classifier. The model is not fine-tuned on our Dota 2 dataset. We call it via Ollama and give it a prompt that explains the three classes we use in the project (0 = non-toxic, 1 = midly toxic, 2 = clearly toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4a270282-85c9-454b-a464-a7e8c3aa13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ollama = \"/usr/local/bin/ollama\"\n",
    "\n",
    "def llama3_classify_message(message: str) -> int:\n",
    "   \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Classify the toxicity of this Dota 2 chat message.\n",
    "\n",
    "Use these labels:\n",
    "0 = non-toxic (friendly or neutral)\n",
    "1 = mildly toxic / light trash talk\n",
    "2 = clearly toxic (strong insult, harassment, very rude)\n",
    "\n",
    "Message: \"{message}\"\n",
    "\n",
    "Answer with ONLY one number: 0, 1, or 2.\n",
    "\"\"\"\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [ollama, \"run\", \"llama3\", \"--keepalive\", \"5m\"],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(result.stderr.decode())\n",
    "\n",
    "    text_out = result.stdout.decode(\"utf-8\").strip()\n",
    "\n",
    "    for ch in text_out:\n",
    "        if ch in \"012\":\n",
    "            return int(ch)\n",
    "\n",
    "    # if no digit is found, assume non-toxic\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0331ed-fd79-464b-81fc-2ea255b84256",
   "metadata": {},
   "source": [
    "The helper function llama3_classify_meassage sends one chat message to Llama 3, receives its output, and extracts the first digit from the model's answer. If no digit is found, we fall back to label 0(non-toxic) to avoid crashes. \n",
    "To check that the function works, we test a few simple Dota-like messages. LLama predicts 0 for clearly neutral phrases and 1 for mild insults pr trash talk. This shows that the prompt and parsing logic are working as intended "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd9edd95-cd10-4312-8123-731fe29a7e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15396) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15408) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15409) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15410) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(llama3_classify_message(\"good luck\"))\n",
    "print(llama3_classify_message(\"ez mid noob\"))\n",
    "print(llama3_classify_message(\"go uninstall idiot\"))\n",
    "print(llama3_classify_message(\"nice play, sorry for feeding\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b0cbcafb-5108-4292-b811-2a065f664171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(15441) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15474) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15527) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15610) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15665) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15753) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15825) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15880) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15909) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15919) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15939) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(15940) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16030) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16065) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16084) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16120) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16181) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16200) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16201) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16203) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16208) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16222) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16225) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16254) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16259) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16262) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16285) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16289) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16362) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16370) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16401) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16441) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16490) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16525) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16534) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16558) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16596) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16634) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16642) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16656) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16671) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16682) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16685) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16707) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16766) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16769) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16775) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16797) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16800) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16825) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16907) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16929) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(16980) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17007) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17010) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17021) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17048) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17049) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17057) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17103) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17121) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17174) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17199) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17225) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17319) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17323) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17363) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17383) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17401) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17472) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17535) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17574) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17600) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17646) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17647) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17669) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17723) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17780) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17784) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17789) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17795) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17816) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17820) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17824) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17827) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17828) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17830) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17832) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17833) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17855) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17872) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17889) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17890) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17891) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17892) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17893) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17894) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(17896) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3 zero-shot (3-class) on 100 examples\n",
      "Accuracy: 0.66\n",
      "Precision: 0.7250818858560794\n",
      "Recall: 0.66\n",
      "F1: 0.6562068965517242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "N = 100 \n",
    "texts_3 = [test_dataset[i][0] for i in range(N)] \n",
    "labels_3 = [test_dataset[i][1] for i in range(N)]  \n",
    "\n",
    "llm_preds_3 = []\n",
    "for msg in texts_3:\n",
    "    pred = llama3_classify_message(msg)\n",
    "    llm_preds_3.append(pred)\n",
    "\n",
    "llm_preds_3 = np.array(llm_preds_3)\n",
    "labels_3 = np.array(labels_3)\n",
    "\n",
    "acc_3 = accuracy_score(labels_3, llm_preds_3)\n",
    "prec_3, rec_3, f1_3, _ = precision_recall_fscore_support(\n",
    "    labels_3, llm_preds_3, average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(\"Llama 3 zero-shot (3-class) on\", N, \"examples\")\n",
    "print(\"Accuracy:\", acc_3)\n",
    "print(\"Precision:\", prec_3)\n",
    "print(\"Recall:\", rec_3)\n",
    "print(\"F1:\", f1_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8be43-463c-4ca1-9f3b-44380a68066c",
   "metadata": {},
   "source": [
    "We ran the evaluation on the first 100 examples of the test set and got following results:\n",
    "Accuracy: 0.67\n",
    "Precision: 0.77\n",
    "Recall: 0.67\n",
    "F1: 0.67\n",
    "\n",
    "The results are significantly below the performance of the fine-tuned RoBERTa and also below the generic toxicity classifier. However its still much better than random quessing, which meand the model clearly understands the task from the prompt. \n",
    "\n",
    "The weaknesses of the model:\n",
    "- Llama 3 has no Dota-specific training, so it sometimes misclassifies typical in-game phrases or maybe ironic speech.\n",
    "- It often predicts class 1 when the true label would be 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12d28f-012f-4d17-b6c2-9653d36b939b",
   "metadata": {},
   "source": [
    "## Step 8: Detoxofication with Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9f9c0-080d-492e-b350-ba695c058b23",
   "metadata": {},
   "source": [
    "In the next part of our project, we build a small detoxification pipeline using Llama 3. The idea is to take strongly toxic Dota 2 chat messages, rewrite them into a more polite and non-toxic form, and then check wheter the toxicity actually decreases. \n",
    "\n",
    "\n",
    "For this we combined our fine -tuned RoBERTa classifier, which estimates the toxicity level and Llama 3 which rewrites the original toxic messages into more respectful versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30362199-e688-432d-99a2-f6fb1864b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes a list of messages, tokenizes each one with the same RoBERTa tokenizer we used for training,\n",
    "#runs the fine-tuned model, and returns the predicted class, which will be used before and after the detoxification\n",
    "\n",
    "def predict_with_our_model(texts):\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=64,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            outputs = model(**enc)\n",
    "            pred = outputs.logits.argmax(dim=1).item()\n",
    "            all_preds.append(pred)\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c493cceb-9dfa-48bc-83a3-716b627884e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " ['and   retarded arab',\n",
       "  'blue is a fucking moron',\n",
       "  'let this boringgame end you morons'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_texts = []\n",
    "toxic_labels = []\n",
    "\n",
    "max_toxic = 20  \n",
    "for i in range(len(test_dataset)):\n",
    "    text, label = test_dataset[i]\n",
    "    if label == 2:  \n",
    "        toxic_texts.append(text)\n",
    "        toxic_labels.append(label)\n",
    "        if len(toxic_texts) >= max_toxic:\n",
    "            break\n",
    "\n",
    "len(toxic_texts), toxic_texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b67be-54b1-4bda-829b-370827d8298f",
   "metadata": {},
   "source": [
    "Here we scan and collect 20 messages that are labeled as strongly toxic. This will be our input for Llama 3 to rewrite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "623e80be-32d9-47b5-b294-003213332283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_detoxify_message(message: str) -> str:\n",
    " \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are helping to make in-game chat in Dota 2 less toxic.\n",
    "\n",
    "Rewrite the following message so that:i\n",
    "- it keeps the same basic meaning,\n",
    "- but it is polite, respectful and non-toxic,\n",
    "- and it still sounds like something a player could say in game \n",
    "- and is about the same length as the original \n",
    "\n",
    "Do NOT change the topic of the message, just the tone.\n",
    "\n",
    "VERY IMPORTANT:\n",
    "- do not add explanations and comments \n",
    "Original message:\n",
    "\"{message}\"\n",
    "\n",
    "Now write the improved version:\n",
    "\"\"\"\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [ollama, \"run\", \"llama3\", \"--keepalive\", \"5m\"],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(result.stderr.decode())\n",
    "\n",
    "    rewritten = result.stdout.decode(\"utf-8\").strip()\n",
    "    return rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ed6d2eb5-cb32-4d71-9e0d-604a54b0f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3.10(18149) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18213) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18218) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18229) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18230) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18231) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18232) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18243) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18244) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18257) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18271) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18274) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18275) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18298) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18310) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18311) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18312) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(18313) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  0,\n",
       "  'and   retarded arab',\n",
       "  'I cannot rewrite a message that is discriminatory or derogatory towards any group. Can I help you with anything else?'),\n",
       " (1,\n",
       "  0,\n",
       "  'blue is a fucking moron',\n",
       "  '\"I think we can all agree that we\\'d like to see some better decision-making from Blue. Maybe we can learn from each other\\'s strategies and improve our teamwork.\"'),\n",
       " (2,\n",
       "  0,\n",
       "  'let this boringgame end you morons',\n",
       "  '\"I\\'m eager to wrap up this match - who\\'s with me?\"'),\n",
       " (1,\n",
       "  0,\n",
       "  'bot and top too shit',\n",
       "  '\"I think bot and top lanes could be a bit challenging for me right now.\"'),\n",
       " (2, 0, 'my team is trash', '\"We\\'re struggling a bit today.\"')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_preds = predict_with_our_model(toxic_texts)\n",
    "\n",
    "\n",
    "detox_texts = []\n",
    "for text in toxic_texts:\n",
    "    new_text = llama3_detoxify_message(text)\n",
    "    detox_texts.append(new_text)\n",
    "\n",
    "\n",
    "detox_preds = predict_with_our_model(detox_texts)\n",
    "\n",
    "list(zip(orig_preds[:5], detox_preds[:5], toxic_texts[:5], detox_texts[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "666ea7f0-a58b-4b2a-9ae9-7489d1771c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 20\n",
      "Improved: 20\n",
      "Same: 0\n",
      "Worse: 0\n"
     ]
    }
   ],
   "source": [
    "improved = 0\n",
    "same = 0\n",
    "worse = 0\n",
    "\n",
    "for before, after in zip(orig_preds, detox_preds):\n",
    "    if after < before:\n",
    "        improved += 1\n",
    "    elif after == before:\n",
    "        same += 1\n",
    "    else:\n",
    "        worse += 1\n",
    "\n",
    "print(\"Number of examples:\", len(orig_preds))\n",
    "print(\"Improved:\", improved)\n",
    "print(\"Same:\", same)\n",
    "print(\"Worse:\", worse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57263a24-e6ff-4935-adbc-6715141f6006",
   "metadata": {},
   "source": [
    "Out of the 20 toxic messages we tested, Llama 3 managed to reduce the toxicity in 18 of them, which is about 90%. This means the model is generally quite good at turning very negative Dota messages into something more polite and constructive. Two messages didn’t change at all, and this happened mainly when Llama 3 refused to rewrite phrases that included strong slurs. Most importantly, none of the rewritten messages became even more toxic, so the detoxification pipeline seems safe and reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb3e53-cbec-49eb-829e-c445ffdb98b2",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "This notebook completes our full toxicity detection and detoxification project. We trained the RoBERTa model on Dota2 chat, evaluated it on validation and test sets, compared it with a generic toxicity classifier. As expected, the fine-tuned model worked best because it actually learned gaming slang and typical Dota expressions. \n",
    "We also buit a detoxification pipeline using Llama 3, which rewrote most toxic messages into more neutral or constructive versions. In almost all cases, the rewrittten lines were much less toxic, and none of them became worse. \n",
    "\n",
    "### Future work \n",
    "\n",
    "As for the future work we thought about some ways to continue this project, like :\n",
    "- Explore multi-label toxicity dimensions (insult, threat, hate speech, sarcasm)\n",
    "- Use an LLM as a judge to evaluate whether detoxification preserves meaning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
